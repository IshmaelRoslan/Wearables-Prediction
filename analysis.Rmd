---
title: "Exercise Skills"
author: "Ishmael Roslan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE,
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5)
```

## Importing Data and Spending the Data Budget

Reading in data from the source.

```{r load_data}
library(tidyverse)
train <-
  read_csv(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
    na = c("", "NA", "#DIV/0!")
  )

test <-
  read_csv(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
    na = c("", "NA", "#DIV/0!")
  )
```

## Feature Extraction

Data Wrangling to ensure features have the appropriate data types.

```{r feature_wrangling}
training <-
  train %>%
  mutate(
    datetime = lubridate::dmy_hms(train$cvtd_timestamp),
    across(where(is.logical), as.numeric),
    new_window = str_replace(new_window, "no", "FALSE"),
    new_window = str_replace(new_window, "yes", "TRUE"),
    new_window = as.logical(new_window)
  ) %>%
  select(-c(raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))

testing <-
  test %>%
  mutate(
    datetime = lubridate::dmy_hms(test$cvtd_timestamp),
    across(where(is.logical), as.numeric),
    new_window = str_replace(new_window, "no", "FALSE"),
    new_window = str_replace(new_window, "yes", "TRUE"),
    new_window = as.logical(new_window)
  ) %>%
  select(-c(raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
```

Generate 10 folds for cross-validation, stratified by `classe`. For a dataset of this size, 10-fold cross-validation should be sufficient to optimise bias-variance trade_off.

```{r 10foldcv}
library(tidymodels)
tidymodels_prefer()
wear_folds <-
  vfold_cv(training, v = 10, strata = classe)
```

## Build Models

Firstly, build a recipe for pre-processing.
1. Remove predictors with at least 90% of data missing (most models do not deal well with missing data)

2. Remove predictors with near zero variance (these will not provide useful information but bloat the , model)

3. Remove highly correlated predictors (as above)

4. Remove predictors related to time or the test subject (I might reconsider putting these back in if the model performs poorly)

5. Normalise numerical predictors (to improve the fitting algorithms)

```{r recipes}
wear_rec <-
  recipe(classe ~ ., data = training) %>%
  update_role("...1", new_role = "id") %>%
  step_filter_missing(all_predictors(), threshold = 0.9) %>%
  step_nzv(all_numeric()) %>%
  step_corr(all_numeric()) %>%
  step_rm(c("...1", "user_name", "new_window", "num_window", "datetime")) %>%
  step_normalize(all_predictors()) %>%
  prep()
```
### Inital Model Testing - Workflow Sets

Specify three initial classification models.

```{r initial_model_specification}
rf_spec <-
  rand_forest(trees = 1000) %>%
  set_engine("ranger") %>%
  set_mode("classification")

tree_spec <-
  boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")

knn_spec <-
  nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")
```

Generate a workflowset for initial fitting
```{r workflowsets}
wear_models <-
  workflow_set(
    preproc = list(wear_rec),
    models = list(rf =  rf_spec,
                  xgb = tree_spec,
                  knn = knn_spec),
    cross = TRUE
  )
```

Fit initial workflow.

```{r fit_workflow}
doParallel::registerDoParallel()
set.seed(1690)
wear_rs <-
  wear_models %>%
  workflow_map("fit_resamples",
               resamples = wear_folds,
               metrics = metric_set(accuracy, kap))
```

### Tuning hyperparameters for the selected model.
Use 10-fold cross-validation to tune the `mtry`  and `min_n` hyperparameters for the random forest model.
```{r initial_tuning}
tune_spec <-
  rand_forest(mtry = tune(),
              trees = 1000,
              min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

tune_wf <- workflow() %>%
  add_recipe(wear_rec) %>%
  add_model(tune_spec)

doParallel::registerDoParallel()
set.seed(1690)

tune_res <-
  tune_grid(tune_wf,
            resamples = wear_folds,
            grid  = 10)
```

Visualise the effect of hyperparameters on roc_auc.
```{r visualise_roc}
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
               values_to = "value",
               names_to = "parameter") %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend =  F) +
  geom_line(alpha = 0.5, size = 1.5) +
  facet_wrap( ~ parameter, scales = "free")
```

Construct a `regular_grid` of hyperparameters over a smaller range than above, to further tune the model.
```{r regular_grid}
rf_grid <-
  grid_regular(mtry(range = c(6, 15)),
               min_n(range = c(2, 5)),
               levels = 40)

set.seed(1690)

regular_res <-
  tune_grid(tune_wf,
            resamples = wear_folds,
            grid  = rf_grid)
```

Visualise the tuning results.
```{r visualise_metrics}
regular_res %>%
  collect_metrics() %>%
  select(mtry:.metric,mean) %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  facet_wrap(~.metric, scales = "free")
```

### Estimating Out of Sample Error
```{r final_model}
best_auc <- select_best(regular_res, "roc_auc")
final_rf <- finalize_model(tune_spec,
                           best_auc)
```
The parameters which provide the best auc metric are mtry = `r as.character(best_auc[1])` and min_n = `r as.character(best_auc[2])`. These parameters resulted in estimated out of sample errors as follows;
```{r OOS_Error}
best_model_id <-
  regular_res %>%
  select_best("roc_auc") %>%
  pull(.config)

OOS <- regular_res %>%
  collect_metrics() %>%
  filter(.config == best_model_id) %>%
  select(mtry:.metric, mean) %>%
  pivot_wider(names_from = ".metric", values_from = "mean")
```
Accuracy: `r OOS[3]`

roc_auc: `r OOS[4]`

The estimated out-of sample error is therefore less than 1 %, suggesting there may be some overfitting.

## Final Model Fit and Predictions.
Complete a final fit on the whole training set.
```{r final_fit}
final_fit <-
  final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(classe ~ .,
      data = juice(wear_rec))
```

Make predictions on the testing set.
```{r predictions}
predict(final_fit,testing)
```

## References
Thanks to the following for access the WLE Dataset.

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

[Read more](http://web.archive.org/web/20161224072740/http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf)

